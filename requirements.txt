# GLM Distributed Llama - Requirements

# Core dependencies
numpy>=1.21.0
torch>=2.0.0
transformers>=4.30.0
sentencepiece>=0.1.99
protobuf>=3.20.0

# Model loading and conversion
accelerate>=0.20.0
safetensors>=0.3.0
tokenizers>=0.13.0

# Hardware detection and monitoring
GPUtil>=1.4.0
psutil>=5.9.0
py3nvml>=0.2.7

# Distributed computing
pyzmq>=25.0.0
click>=8.1.0
tqdm>=4.65.0

# HTTP requests and APIs
requests>=2.28.0
aiohttp>=3.8.0
fastapi>=0.100.0
uvicorn>=0.22.0

# Development dependencies
pytest>=7.0.0
pytest-asyncio>=0.21.0
black>=23.0.0
flake8>=6.0.0
mypy>=1.0.0

# Optional dependencies for enhanced features
# Uncomment if needed:

# CUDA support (if using NVIDIA GPUs)
# torch-audio>=2.0.0  # Only if CUDA available
# nvidia-ml-py3>=7.352.0  # For GPU monitoring

# Visualization and monitoring
matplotlib>=3.5.0
seaborn>=0.11.0
wandb>=0.15.0  # For experiment tracking

# Web interface
streamlit>=1.25.0
gradio>=3.35.0

# Mobile deployment
onnx>=1.14.0
onnxruntime>=1.15.0
